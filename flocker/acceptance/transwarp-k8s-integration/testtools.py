# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Testing infrastructure for integration tests.
"""

import json
import os
from itertools import repeat
from os import environ
from functools import wraps
from subprocess import STDOUT, PIPE, Popen
from pyrsistent import PClass, field
from unittest import SkipTest

from twisted.internet.defer import DeferredList, Deferred
from twisted.internet.endpoints import ProcessEndpoint
from twisted.internet.endpoints import connectProtocol
from twisted.internet.error import ProcessTerminated, ConnectionDone
from twisted.internet.protocol import Protocol
from twisted.python.filepath import FilePath
from twisted.internet import reactor
from eliot import Field, MessageType, Logger

from ...common import loop_until
from ..testtools import Cluster, connected_cluster


class MultiFlockerControlPod(Exception):
    """
    multiple flocker control pod on k8s cluster
    """


class MaximumRetryExceed(Exception):
    """
    multiple flocker control pod on k8s cluster
    """


class KubeCommandFailed(Exception):
    """The ``kube`` command failed for some reasons."""


_KUBE_COMMAND = Field.forTypes(
    "kube_command", [bytes], u"The command which was run.")
_OUTPUT = Field.forTypes(
    "output", [bytes], u"The output generated by the command.")
_INPUT = Field.forTypes(
    "input", [bytes], u"The input generated by the command.")
_ERROR = Field.forTypes(
    "error", [bytes], u"The error generated by the command.")
_STATUS = Field.forTypes(
    "status", [int], u"The exit status of the command")

KUBE_ERROR = MessageType(
    "kubernetes:error", [_KUBE_COMMAND, _INPUT, _OUTPUT, _ERROR, _STATUS],
    u"The kubernetes command signaled an error.")


class BadArguments(Exception):
    """
    Bad Arguments
    """

    def __init__(self, cmd, args):
        self._cmd = cmd
        self._args = args

    def __str__(self):
        return "cmd: %s arguments: %s" % (self._cmd, self._args)


class _AccumulatingProtocol(Protocol):
    """
    Accumulate all received bytes.
    """

    def __init__(self):
        self._result = Deferred()
        self._data = b""

    def dataReceived(self, data):
        self._data += data

    def connectionLost(self, reason):
        if reason.check(ConnectionDone):
            self._result.callback(self._data)
        elif reason.check(ProcessTerminated) and reason.value.exitCode == 1:
            self._result.errback(KubeCommandFailed)
        elif reason.check(ProcessTerminated) and reason.value.exitCode == 2:
            self._result.errback(BadArguments())
        else:
            self._result.errback(reason)
        del self._result


def _kube_command(_reactor, cmd, arguments):
    """
    Asynchronously run the ``kube`` command-line tool with the given arguments.

    :param _reactor: A ``IReactorProcess`` provider.

    :param arguments: A ``list`` of ``bytes``, command-line arguments to
    ``kube``.

    :return: A :class:`Deferred` firing with the bytes of the result (on
        exit code 0), or errbacking with :class:`CommandFailed` or
        :class:`BadArguments` depending on the exit code (1 or 2).
    """
    endpoint = ProcessEndpoint(_reactor, cmd, [cmd] + arguments,
                               os.environ)
    d = connectProtocol(endpoint, _AccumulatingProtocol())
    d.addCallback(lambda protocol: protocol._result)

    logger = Logger()

    def async_command_failed(reason, args):
        message = KUBE_ERROR(
            kube_command=args,
            output=str(reason), status=1
        )
        message.write(logger)
        raise KubeCommandFailed

    d.addErrback(async_command_failed, cmd.join(arguments))
    return d


def sync_kube_command(arguments, logger, input=None):
    """
    Synchronously run a command-line tool with the given arguments.

    :param arguments: A ``list`` of ``bytes``, command-line arguments to
        execute.

    :param eliot.Logger logger: The log writer to use to log errors running the command.

    :param input: input content send to stdin
    """
    log_arguments = b" ".join(arguments)
    try:
        process = Popen(arguments, stdout=PIPE, stderr=PIPE, stdin=PIPE)
        output, error = process.communicate(input=input)
    except Exception as e:
        KUBE_ERROR(
            kube_command=log_arguments, input=input, output=str(e), status=1).write(logger)
        raise KubeCommandFailed
    if error != "":
        KUBE_ERROR(
            kube_command=log_arguments, input=input, output=output, error=error, status=2).write(logger)
        raise KubeCommandFailed
    return output


class FlockerPVProvisionFailed(Exception):
    """
    Failed to provision flocker PersistentVolume
    """


class FlockerPVCProvisionFailed(Exception):
    """
    Failed to provision flocker PersistentVolumeClaim
    """


class MultiFlockerDataset(Exception):
    """
    Mutliple flocker dataset existed for the same name
    """

    def __init__(self, dataset):
        self._dataset = dataset

    def __str__(self):
        return "Multiple flocker dataset existed for %s" % self._dataset


class FlockerDatasetNotExist(Exception):
    """
    Flocker Dataset not exist in configuration

    """


def get_filed_value_from_json(content, json_field):
    config = json.loads(content)
    return config[json_field]


def get_only_dataset(datasets):
    if len(datasets) > 1:
        raise MultiFlockerDataset
    if len(datasets) == 0:
        raise FlockerDatasetNotExist
    return datasets[0]


def selector_string(selector):
    """
    Convert selector from map to kvs
    :param selector: a map of selector
    :return kvs seperate by delimer ','
    """
    selector_str = ""
    for key, value in selector.iteritems():
        selector_str += "{}={},".format(key, value)
    return selector_str[:len(selector_str) - 1]


def pod_deleted(pod):
    """
    Check if pod is deleted
    :param pod: check if pod is deleted
    :return bool: return true if pod deletionTimeStamp is set
    """
    if b'deletionTimestamp' in pod[b'metadata']:
        return True
    return False


class K8SCluster(PClass):
    """
    A record of the apiserver and flocker cluster for acceptance testing.

    :ivar Cluster flocker_cluster: The flocker cluster

    :ivar kube_config FilePath: The kube_config of k8s cluster.

    """

    flocker_cluster = field(mandatory=True, type=Cluster)
    kube_config = field(mandatory=True, type=FilePath)
    logger = Logger()

    def kubectl_create_with_stdin(self, input):
        """
        Run kubectl create command, and receive request body from stdin.
        This is used to create pvcset, replication_controller, pods
        """
        return sync_kube_command(["kubectl", "--kubeconfig=%s" % self.kube_config.path, "create", "-f", "-"],
                                 logger=self.logger,
                                 input=input)

    def kubectl_delete_with_stdin(self, input):
        """
        Run kubectl delete command, and receive request body from stdin.
        This is used to create pvcset, replication_controller, pods
        """
        return sync_kube_command(["kubectl", "--kubeconfig=%s" % self.kube_config.path, "delete", "-f", "-"],
                                 logger=self.logger,
                                 input=input)

    def kubectl_get(self, args):
        """
        Run kubectl
        :param args: A ``list`` of ``bytes``, command-line arguments to ``kubectl``.

        :return a Deffered fired with kubectl command
        """
        d = _kube_command(reactor, "kubectl", ["--kubeconfig={}".format(self.kube_config.path)] + args)
        return d.addCallback(lambda content: json.loads(content))

    def create_pvcset(self, pvcset_request):
        """
        Create pvcset and check provision status
        :param pvcset_request: json request body send to kubernetes
        """
        return self.kubectl_create_with_stdin(pvcset_request)

    def check_pvcset_provision(self, desire_replicas, selector):
        """
        Wait until desire_replicas of pvc being provisioned, PVCs is selected with selector

        :param desire_replicas: desire replicas of pvcset
        :param selector: a map of key-value used to select PVCs

        :return a list of tuples: a list of tuples from pvc name to flocker dataset id
        """
        selector_str = selector_string(selector)
        def wait_for_replicas():
            pvc = self.kubectl_get(["get", "pvc", "-l", selector_str, "-o", "json"])
            return pvc.addCallback(lambda results: len(results['items']) >= desire_replicas)

        cmd = loop_until(reactor, wait_for_replicas)

        def check_pvc(ignore):
            pvc = self.kubectl_get(["get", "pvc", "-l", selector_str, "-o", "json"])

            def got_results(results):
                deffer_list = []
                for item in results["items"]:
                    pvc_name = item[b'metadata'][b'name']
                    deffer_list.append(self.check_pvc_provision(pvc_name))
                dl = DeferredList(deffer_list, consumeErrors=False)

                def get_dl(ids):
                    dataset_list = []
                    for (success, tup) in ids:
                        if success:
                            dataset_list.append(tup)
                        else:
                            continue
                    return dataset_list

                return dl.addCallback(get_dl)

            return pvc.addCallback(got_results)

        return cmd.addCallback(check_pvc)

    def check_pvc_provision(self, name):
        """
        Wait until pvc being provisioned, return if failed or success

        :param name: name of pvc
        :return unicode: dataset_id
        """

        def check_pvc_bound():
            cmd = self.kubectl_get(["get", "pvc", name, "-o", "json"])
            # pvc will block at Pending status if provision failed
            cmd.addCallback(lambda res: res[b'status'][b'phase'] == b"Bound")
            return cmd

        pvc = loop_until(reactor, check_pvc_bound, repeat(1, 120))

        def get_pvc_name(ignore):
            cmd = self.kubectl_get(["get", "pvc", name, "-o", "json"])
            return cmd.addCallback(lambda res: self.check_pv_provision(res[b'spec'][b'volumeName']))

        pvc.addCallback(get_pvc_name)
        return pvc.addCallback(lambda dataset_name: (name, dataset_name))

    def check_pv_provision(self, name):
        """
        Wait unil pv being provisioned, return if failed or success

        :param name: name of pv
        :return unicode: dataset_id
        """

        def check_pv_bound():
            cmd = self.kubectl_get(["get", "pv", name, "-o", "json"])

            def check_status(res):
                status = res[b'status'][b'phase']
                if status == b"Failed":
                    raise FlockerPVProvisionFailed()
                return status == b"Bound"

            cmd.addCallback(check_status)
            return cmd

        pv = loop_until(reactor, check_pv_bound)

        def get_pv_name(ignore):
            d = self.kubectl_get(["get", "pv", name, "-o", "json"])
            return d.addCallback(lambda res: res[b'spec'][b'flocker'][b'datasetName'])

        return pv.addCallback(get_pv_name)

    def get_flocker_dataset_id(self, dataset_name):
        """
        Get dataset_id from flocker. raise exeception if multiple match

        :param dataset_name: name of flocker dataset

        :return id: id of flocker dataset
        """
        ids = self.flocker_cluster.get_datasetid_by_name(dataset_name)
        return ids.addCallback(self._get_only_dataset)

    def check_flocker_dataset(self, dataset_name):
        datasets = self.flocker_cluster.get_datasetid_by_name(dataset_name)

        datasets.addCallback(self._get_only_dataset)

        def _check_zfs_on_nodes(uuid, node, _dataset_id):
            def _run_check():
                command = ["zfs", "list", "|", "grep", "%s.default.%s" % (uuid, _dataset_id)]
                d = node.run_in_flocker_container_as_root(command)

                def not_existing(failure):
                    failure.trap(ProcessTerminated)
                    return False

                return d.addCallbacks(lambda result: True, not_existing)

            return loop_until(reactor, _run_check, repeat(1, 120))

        def check_on_nodes(dataset):
            for node in self.flocker_cluster.nodes:
                if node.uuid == dataset.primary:
                    config = node.run_in_flocker_container_as_root(["cat", "/etc/flocker/volume.json"])
                    config.addCallback(lambda content: get_filed_value_from_json(content, u"uuid"))
                    config.addCallback(_check_zfs_on_nodes, node)
                    return config.addCallback(_check_zfs_on_nodes, node, datasets.dataset_id)

        return datasets.addCallback(check_on_nodes)

    def create_replication_controllers(self, replication_controller):
        """
        Create pods with rc and bind to pvc, check bind status
        :param unicode replication_controller: json request body send to kubernetes
        """
        self.kubectl_create_with_stdin(replication_controller)

    def check_rc_provision(self, desire_replicas, selector):
        """
        Check rc provision status, return util successfully created
        :param desire_replicas: desire replicas of rc
        :param selector: a map of kv used to select pods belong to this rc
        """
        selector_str = selector_string(selector)

        def wait_for_replicas():
            pvc = self.kubectl_get(["get", "pod", "-l", selector_str, "-o", "json"])
            pvc.addCallback(lambda results: [pod for pod in results[b'items']
                                             if b'deletionTimestamp' not in pod[b'metadata']])
            return pvc.addCallback(lambda results: len(results) >= desire_replicas)

        cmd = loop_until(reactor, wait_for_replicas)

        def check_rc(ignore):
            pvc = self.kubectl_get(["get", "pod", "-l", selector_str, "-o", "json"])

            def got_results(results):
                deffer_list = []
                for item in results["items"]:
                    if pod_deleted(item):
                        continue
                    pod_name = item[b'metadata'][b'name']
                    deffer_list.append(self.check_pod_provision(pod_name))
                dl = DeferredList(deffer_list, consumeErrors=False)

                def get_dl(ids):
                    dataset_list = []
                    for (success, tup) in ids:
                        if success:
                            dataset_list.append(tup)
                        else:
                            continue
                    return dataset_list

                return dl.addCallback(get_dl)

            return pvc.addCallback(got_results)

        return cmd.addCallback(check_rc)

    def check_pod_provision(self, pod_name):
        """
        Check pod provision status, return until pod successfully created
        :param pod_name: pod name
        """
        # TODO: check pod bind status on the node, including zfs creation, pvc binding
        def pod_ready():
            pod = self.kubectl_get(["get", "pod", pod_name, "-o", "json"])
            return pod.addCallback(lambda res: res[b'status'][b'phase'] == b'Running')
        return loop_until(reactor, pod_ready, repeat(1, 240))

    def delete_pvcset(self, request):
        """
        Delete pvc
        :param request: request body of pvcset
        """
        self.kubectl_delete_with_stdin(request)

    def delete_rc(self, request):
        """
        Delete rc
        :param request: request body of rc
        """
        self.kubectl_delete_with_stdin(request)

    def move_dataset(self):
        """
        Create single pod, and migrate the pod
        """
        return None


def generate_kube_config(path="/tmp/flocker_k8s_kubeconfig"):
    """
    Generate kube config file
    """
    host = environ.get("KUBERNETES_SERVICE_HOST")

    if host is None:
        raise SkipTest(
            "Set kubernetes apiserver host using KUBERNETES_SERVICE_HOST environment variable.")

    port = environ.get("KUBERNETES_SERVICE_PORT")
    sync_kube_command([b"kubectl", "config", "--kubeconfig={}".format(path), "set-cluster", "flocker-trial-test", "--server=http://{}:{}".format(host, port), "--insecure-skip-tls-verify=true"], logger=None)
    sync_kube_command([b"kubectl", "config", "--kubeconfig={}".format(path), "set-context", "flocker-trial-test", "--cluster=flocker-trial-test"], logger=None)
    sync_kube_command([b"kubectl", "config", "--kubeconfig={}".format(path), "use-context", "flocker-trial-test"], logger=None)


def _load_json(content):
    return json.loads(content)


def _get_test_k8s_cluster(_reactor, logger):
    """
    Build a k8s cluster instance

    :returns: A ``Deferred`` which fires with a ``k8s_Cluster`` instance.
    """
    # loop until require nodes is available on target cluster
    # 1. generate kube_config if not existed
    # 2. loop until flocker-control pod is up
    # 3. loop until required numbers of flocker-agent is up
    config_path = FilePath("/tmp/flocker_k8s_config")
    if not config_path.exists():
        generate_kube_config(config_path.path)

    # use label selector to find out flocker-control node
    def get_flocker_control_pod():
        kube_cmd = _kube_command(_reactor,
                                 "kubectl",
                                 ["--kubeconfig=%s" % config_path.path,
                                  "get", "pods",
                                  "-o", "json",
                                  "-l", "name=flocker-control"])

        kube_cmd.addCallback(lambda content: json.loads(content))

        def get_flocker_control_ip(output):
            control_nodes = len(output[u"items"])
            if control_nodes == 0:
                return False
            if control_nodes > 1:
                raise MultiFlockerControlPod
            control_pod = output["items"][0]
            return bytes(control_pod["status"]["podIP"])

        kube_cmd.addCallback(get_flocker_control_ip)
        return kube_cmd

    flocker_control = loop_until(_reactor, get_flocker_control_pod)

    def init_k8s_cluster(control_ip):
        agent_nodes_env_var = environ.get('FLOCKER_ACCEPTANCE_NUM_AGENT_NODES')

        if agent_nodes_env_var is None:
            raise SkipTest(
                "Set the number of configured acceptance testing nodes using the "
                "FLOCKER_ACCEPTANCE_NUM_AGENT_NODES environment variable.")

        num_agent_nodes = int(agent_nodes_env_var)

        certificates_path = FilePath(
            environ["FLOCKER_ACCEPTANCE_API_CERTIFICATES_PATH"])

        hostname_to_public_address_env_var = environ.get(
            "FLOCKER_ACCEPTANCE_HOSTNAME_TO_PUBLIC_ADDRESS", "{}")
        hostname_to_public_address = json.loads(hostname_to_public_address_env_var)

        flocker_cluster = connected_cluster(
            reactor,
            control_ip,
            certificates_path,
            num_agent_nodes,
            hostname_to_public_address,
            u'apiuser'
        )
        return flocker_cluster.addCallback(
            lambda cluster: K8SCluster(flocker_cluster=cluster, kube_config=config_path))

    return flocker_control.addCallback(init_k8s_cluster)


def require_k8s_cluster(num_nodes):
    """
    A decorator which will call the supplied test_method when a k8s_cluster with
    the required number of nodes is available.

    :param int num_nodes: The number of nodes that are required in the k8s_cluster.

    """
    def decorator(test_method):
        """
        :param test_method: The test method that will be called when the
            cluster is available and which will be supplied with the
            ``k8s_cluster``keyword argument.
        """

        def call_test_method_with_k8s_cluster(k8s_cluster, test_case, args, kwargs):
            kwargs['k8s_cluster'] = k8s_cluster
            return test_method(test_case, *args, **kwargs)

        @wraps(test_method)
        def wrapper(test_case, *args, **kwargs):
            # Check that the required number of nodes are reachable and
            # clean them up prior to the test.  The nodes must already
            # started before we clean them
            waiting_for_cluster = _get_test_k8s_cluster(reactor, logger=None)

            def clean(k8s_cluster):
                flocker_cluster = k8s_cluster.flocker_cluster
                existing = len(flocker_cluster.nodes)
                if num_nodes > existing:
                    raise SkipTest(
                        "This test requires a minimum of {necessary} nodes, "
                        "{existing} node(s) are set.".format(
                            necessary=num_nodes, existing=existing))
                return k8s_cluster

            # TODO: add cluster state check before run test
            waiting_for_cluster.addCallback(clean)
            calling_test_method = waiting_for_cluster.addCallback(
                call_test_method_with_k8s_cluster,
                test_case, args, kwargs
            )
            return calling_test_method

        return wrapper

    return decorator
